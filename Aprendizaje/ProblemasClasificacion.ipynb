{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad: Problemas de clasificación \n",
    "\n",
    "En este ejercicio trabajarás con el conjunto de datos que se te asignó de acuerdo al último número de tu matrícula (ver las notas del ejercicio). En estos archivos se tienen datos procesados de un experimento de psicología en el que se mide la respuesta cerebral cuando un sujeto presta atención a un estímulo visual que aparece de manera repentina y cuando no presta atención a dicho estímulo visual. Los datos están en archivos de texto, los cuales se cargan con la función loadtxt de numpy\n",
    "\n",
    "La primera columna corresponde a la clase (1 o 2). La clase 1 representa cuando el sujeto está prestando atención, y la clase 2 cuando no lo hace. La segunda columna se ignora, mientras que el resto de las columnas indican las variables que se calcularon de la respuesta cerebral medida con la técnicas de Electroencefaolografía para cada caso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, classification_report, accuracy_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>V146</th>\n",
       "      <th>V147</th>\n",
       "      <th>V148</th>\n",
       "      <th>V149</th>\n",
       "      <th>V150</th>\n",
       "      <th>V151</th>\n",
       "      <th>V152</th>\n",
       "      <th>V153</th>\n",
       "      <th>V154</th>\n",
       "      <th>V155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.983609</td>\n",
       "      <td>-2.307221</td>\n",
       "      <td>-1.106408</td>\n",
       "      <td>0.125453</td>\n",
       "      <td>0.571937</td>\n",
       "      <td>0.724484</td>\n",
       "      <td>1.005067</td>\n",
       "      <td>0.958374</td>\n",
       "      <td>0.165433</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.437280</td>\n",
       "      <td>-0.052512</td>\n",
       "      <td>1.044647</td>\n",
       "      <td>0.653567</td>\n",
       "      <td>0.286931</td>\n",
       "      <td>1.371024</td>\n",
       "      <td>2.635984</td>\n",
       "      <td>1.972739</td>\n",
       "      <td>-0.191824</td>\n",
       "      <td>-1.405858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.819334</td>\n",
       "      <td>-0.187179</td>\n",
       "      <td>-0.604254</td>\n",
       "      <td>0.162191</td>\n",
       "      <td>0.885844</td>\n",
       "      <td>0.279768</td>\n",
       "      <td>-0.972527</td>\n",
       "      <td>-1.170367</td>\n",
       "      <td>-0.190216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232946</td>\n",
       "      <td>1.205306</td>\n",
       "      <td>0.739159</td>\n",
       "      <td>0.015158</td>\n",
       "      <td>0.549055</td>\n",
       "      <td>1.975111</td>\n",
       "      <td>2.554563</td>\n",
       "      <td>1.585072</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>0.181487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.412824</td>\n",
       "      <td>0.367176</td>\n",
       "      <td>0.585049</td>\n",
       "      <td>0.301655</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.049085</td>\n",
       "      <td>-0.317552</td>\n",
       "      <td>-1.146371</td>\n",
       "      <td>-1.836088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345102</td>\n",
       "      <td>0.100283</td>\n",
       "      <td>0.071815</td>\n",
       "      <td>-1.020441</td>\n",
       "      <td>-2.019527</td>\n",
       "      <td>-1.743849</td>\n",
       "      <td>-0.693552</td>\n",
       "      <td>-0.224807</td>\n",
       "      <td>-0.620347</td>\n",
       "      <td>-0.973043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.392490</td>\n",
       "      <td>-0.255365</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>1.069737</td>\n",
       "      <td>1.562800</td>\n",
       "      <td>1.106500</td>\n",
       "      <td>-0.064497</td>\n",
       "      <td>-1.145345</td>\n",
       "      <td>-1.308064</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048378</td>\n",
       "      <td>-0.084662</td>\n",
       "      <td>1.100433</td>\n",
       "      <td>1.133709</td>\n",
       "      <td>-0.027949</td>\n",
       "      <td>-1.025664</td>\n",
       "      <td>-0.824307</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.425066</td>\n",
       "      <td>-0.015195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.501571</td>\n",
       "      <td>-0.337430</td>\n",
       "      <td>-0.697348</td>\n",
       "      <td>0.656165</td>\n",
       "      <td>2.088990</td>\n",
       "      <td>1.504034</td>\n",
       "      <td>-0.465597</td>\n",
       "      <td>-1.391680</td>\n",
       "      <td>-0.611330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530430</td>\n",
       "      <td>1.530316</td>\n",
       "      <td>1.472383</td>\n",
       "      <td>0.564503</td>\n",
       "      <td>-0.349507</td>\n",
       "      <td>-0.717304</td>\n",
       "      <td>-0.618919</td>\n",
       "      <td>-0.346143</td>\n",
       "      <td>-0.000279</td>\n",
       "      <td>0.337508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.443020</td>\n",
       "      <td>0.200899</td>\n",
       "      <td>1.511965</td>\n",
       "      <td>2.073191</td>\n",
       "      <td>1.242577</td>\n",
       "      <td>-0.076725</td>\n",
       "      <td>-0.828760</td>\n",
       "      <td>-1.187531</td>\n",
       "      <td>-1.851632</td>\n",
       "      <td>...</td>\n",
       "      <td>1.840325</td>\n",
       "      <td>3.184403</td>\n",
       "      <td>2.494080</td>\n",
       "      <td>0.940529</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.804459</td>\n",
       "      <td>0.921112</td>\n",
       "      <td>0.304525</td>\n",
       "      <td>0.137157</td>\n",
       "      <td>1.066532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>2</td>\n",
       "      <td>1.530154</td>\n",
       "      <td>1.070188</td>\n",
       "      <td>0.328849</td>\n",
       "      <td>-0.250447</td>\n",
       "      <td>-0.140749</td>\n",
       "      <td>0.526235</td>\n",
       "      <td>1.015146</td>\n",
       "      <td>0.838786</td>\n",
       "      <td>0.256393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174581</td>\n",
       "      <td>-0.328666</td>\n",
       "      <td>-0.530457</td>\n",
       "      <td>-0.620668</td>\n",
       "      <td>-1.003254</td>\n",
       "      <td>-1.711879</td>\n",
       "      <td>-2.078918</td>\n",
       "      <td>-1.650707</td>\n",
       "      <td>-0.799395</td>\n",
       "      <td>-0.114377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.007095</td>\n",
       "      <td>0.041654</td>\n",
       "      <td>0.069534</td>\n",
       "      <td>0.228875</td>\n",
       "      <td>0.381789</td>\n",
       "      <td>0.223283</td>\n",
       "      <td>-0.327042</td>\n",
       "      <td>-0.882752</td>\n",
       "      <td>-0.847323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.827518</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>0.991450</td>\n",
       "      <td>0.597160</td>\n",
       "      <td>-0.462990</td>\n",
       "      <td>-0.961375</td>\n",
       "      <td>-0.717605</td>\n",
       "      <td>-0.565029</td>\n",
       "      <td>-0.752179</td>\n",
       "      <td>-0.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.970890</td>\n",
       "      <td>-0.468173</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>1.708848</td>\n",
       "      <td>1.448626</td>\n",
       "      <td>0.512831</td>\n",
       "      <td>-0.025961</td>\n",
       "      <td>0.115387</td>\n",
       "      <td>0.248835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229371</td>\n",
       "      <td>-0.172276</td>\n",
       "      <td>-0.056905</td>\n",
       "      <td>-0.007255</td>\n",
       "      <td>-0.380686</td>\n",
       "      <td>-0.994331</td>\n",
       "      <td>-1.169496</td>\n",
       "      <td>-0.726270</td>\n",
       "      <td>-0.288457</td>\n",
       "      <td>-0.317863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.082099</td>\n",
       "      <td>-0.094875</td>\n",
       "      <td>0.142382</td>\n",
       "      <td>0.211950</td>\n",
       "      <td>-0.267232</td>\n",
       "      <td>-0.953822</td>\n",
       "      <td>-1.075463</td>\n",
       "      <td>-0.471753</td>\n",
       "      <td>0.176050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101385</td>\n",
       "      <td>-0.757957</td>\n",
       "      <td>-1.084154</td>\n",
       "      <td>-0.419822</td>\n",
       "      <td>0.596405</td>\n",
       "      <td>0.975931</td>\n",
       "      <td>0.547534</td>\n",
       "      <td>-0.154162</td>\n",
       "      <td>-0.877143</td>\n",
       "      <td>-1.731017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1393 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1        V3        V4        V5        V6        V7        V8  \\\n",
       "0      1 -1.983609 -2.307221 -1.106408  0.125453  0.571937  0.724484   \n",
       "1      1  0.819334 -0.187179 -0.604254  0.162191  0.885844  0.279768   \n",
       "2      1 -0.412824  0.367176  0.585049  0.301655  0.099057  0.049085   \n",
       "3      1  0.392490 -0.255365  0.142369  1.069737  1.562800  1.106500   \n",
       "4      1  0.501571 -0.337430 -0.697348  0.656165  2.088990  1.504034   \n",
       "...   ..       ...       ...       ...       ...       ...       ...   \n",
       "1388   2 -0.443020  0.200899  1.511965  2.073191  1.242577 -0.076725   \n",
       "1389   2  1.530154  1.070188  0.328849 -0.250447 -0.140749  0.526235   \n",
       "1390   2 -0.007095  0.041654  0.069534  0.228875  0.381789  0.223283   \n",
       "1391   2 -0.970890 -0.468173  0.797872  1.708848  1.448626  0.512831   \n",
       "1392   2 -0.082099 -0.094875  0.142382  0.211950 -0.267232 -0.953822   \n",
       "\n",
       "            V9       V10       V11  ...      V146      V147      V148  \\\n",
       "0     1.005067  0.958374  0.165433  ... -1.437280 -0.052512  1.044647   \n",
       "1    -0.972527 -1.170367 -0.190216  ...  0.232946  1.205306  0.739159   \n",
       "2    -0.317552 -1.146371 -1.836088  ... -0.345102  0.100283  0.071815   \n",
       "3    -0.064497 -1.145345 -1.308064  ... -1.048378 -0.084662  1.100433   \n",
       "4    -0.465597 -1.391680 -0.611330  ...  0.530430  1.530316  1.472383   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1388 -0.828760 -1.187531 -1.851632  ...  1.840325  3.184403  2.494080   \n",
       "1389  1.015146  0.838786  0.256393  ... -0.174581 -0.328666 -0.530457   \n",
       "1390 -0.327042 -0.882752 -0.847323  ... -0.827518  0.183258  0.991450   \n",
       "1391 -0.025961  0.115387  0.248835  ... -0.229371 -0.172276 -0.056905   \n",
       "1392 -1.075463 -0.471753  0.176050  ...  0.101385 -0.757957 -1.084154   \n",
       "\n",
       "          V149      V150      V151      V152      V153      V154      V155  \n",
       "0     0.653567  0.286931  1.371024  2.635984  1.972739 -0.191824 -1.405858  \n",
       "1     0.015158  0.549055  1.975111  2.554563  1.585072  0.314870  0.181487  \n",
       "2    -1.020441 -2.019527 -1.743849 -0.693552 -0.224807 -0.620347 -0.973043  \n",
       "3     1.133709 -0.027949 -1.025664 -0.824307  0.063200  0.425066 -0.015195  \n",
       "4     0.564503 -0.349507 -0.717304 -0.618919 -0.346143 -0.000279  0.337508  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1388  0.940529  0.354632  0.804459  0.921112  0.304525  0.137157  1.066532  \n",
       "1389 -0.620668 -1.003254 -1.711879 -2.078918 -1.650707 -0.799395 -0.114377  \n",
       "1390  0.597160 -0.462990 -0.961375 -0.717605 -0.565029 -0.752179 -0.425400  \n",
       "1391 -0.007255 -0.380686 -0.994331 -1.169496 -0.726270 -0.288457 -0.317863  \n",
       "1392 -0.419822  0.596405  0.975931  0.547534 -0.154162 -0.877143 -1.731017  \n",
       "\n",
       "[1393 rows x 154 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load database\n",
    "df = pd.read_csv('/home/alanv/Documents/7/omar/P1_1.txt', delimiter='\\t', header=None)\n",
    "column_names = [f\"V{i}\" for i in range(1, df.shape[1] + 1)]\n",
    "df.columns = column_names\n",
    "df.drop(['V2','V156'],inplace=True,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Determina si es necesario balancear los datos. En caso de que sea afirmativo, en todo este ejercicio tendrás que utilizar alguna estrategia para mitigar el problema de tener una muestra desbalanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_counts = df['V1'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "value_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Value Counts in dependient Variable')\n",
    "plt.xlabel('Unique Values')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos estan desbalanceados por lo que se opta por balancear por el metodo Weighted loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Evalúa al menos 5 modelos de clasificación distintos utilizando validación cruzada, y determina cuál de ellos es el más efectivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializate x and y\n",
    "x = df.iloc[:, 1:].values\n",
    "y = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Linear SVM -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.87      0.75       278\n",
      "           2       0.97      0.89      0.93      1115\n",
      "\n",
      "    accuracy                           0.89      1393\n",
      "   macro avg       0.81      0.88      0.84      1393\n",
      "weighted avg       0.91      0.89      0.89      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1393, 153)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"-----  Linear SVM -----\")\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "clf = SVC(kernel = 'linear', class_weight='balanced')\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    # Training phase\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Test phase\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RBF-SVM -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.81      0.81       278\n",
      "           2       0.95      0.96      0.95      1115\n",
      "\n",
      "    accuracy                           0.93      1393\n",
      "   macro avg       0.89      0.88      0.88      1393\n",
      "weighted avg       0.93      0.93      0.93      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----- RBF-SVM -----')\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "clf = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    # Training phase\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Test phase\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Decision tree -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.55      0.56       278\n",
      "           2       0.89      0.90      0.89      1115\n",
      "\n",
      "    accuracy                           0.83      1393\n",
      "   macro avg       0.73      0.72      0.73      1393\n",
      "weighted avg       0.82      0.83      0.83      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "print('----- Decision tree -----')\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = DecisionTreeClassifier(class_weight='balanced')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "\n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Random Forest -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.38      0.55       278\n",
      "           2       0.87      0.99      0.93      1115\n",
      "\n",
      "    accuracy                           0.87      1393\n",
      "   macro avg       0.91      0.69      0.74      1393\n",
      "weighted avg       0.88      0.87      0.85      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print('----- Random Forest -----')\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = RandomForestClassifier(class_weight='balanced')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "\n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Logistic Regression -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.85      0.75       278\n",
      "           2       0.96      0.90      0.93      1115\n",
      "\n",
      "    accuracy                           0.89      1393\n",
      "   macro avg       0.81      0.87      0.84      1393\n",
      "weighted avg       0.90      0.89      0.89      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----- Logistic Regression -----')\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = LogisticRegression(class_weight='balanced')\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo mas efectivo es el rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Implementa desde cero el método de regresión logística, y evalúalo con el conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Logistic Regression No Library-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.20      0.97      0.33       278\n",
      "           2       0.56      0.01      0.02      1115\n",
      "\n",
      "    accuracy                           0.20      1393\n",
      "   macro avg       0.38      0.49      0.17      1393\n",
      "weighted avg       0.48      0.20      0.08      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.0001, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0.01  # Initialize to a small non-zero value\n",
    "\n",
    "        # Gradient descent for 'num_iterations' iterations\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.2 else 2 for i in y_predicted]  \n",
    "        return y_predicted_cls\n",
    "\n",
    "# Declare data\n",
    "x = df.iloc[:, 1:].values\n",
    "y = df.iloc[:,0].values\n",
    "\n",
    "\n",
    "print('----- Logistic Regression No Library-----')\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = LogisticRegression(learning_rate=0.001, num_iterations=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    4. Con alguno de los clasificadores que probaste en los pasos anteriores, determina el número óptimo de características utilizando un método tipo Filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciono el metodo rbf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Optimal selection of number of features -----\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]\n",
      "---- n features = 1\n",
      "ACC: 0.764544493437508\n",
      "---- n features = 2\n",
      "ACC: 0.7774863979783915\n",
      "---- n features = 3\n",
      "ACC: 0.7932363786390242\n",
      "---- n features = 4\n",
      "ACC: 0.8082669348392255\n",
      "---- n features = 5\n",
      "ACC: 0.8133518991258606\n",
      "---- n features = 6\n",
      "ACC: 0.8305639359480157\n",
      "---- n features = 7\n",
      "ACC: 0.8291199298625616\n",
      "---- n features = 8\n",
      "ACC: 0.8276965524354709\n",
      "---- n features = 9\n",
      "ACC: 0.8592764498078956\n",
      "---- n features = 10\n",
      "ACC: 0.8628787292746447\n",
      "---- n features = 11\n",
      "ACC: 0.8636136252288491\n",
      "---- n features = 12\n",
      "ACC: 0.8593048142131456\n",
      "---- n features = 13\n",
      "ACC: 0.8772233825842551\n",
      "---- n features = 14\n",
      "ACC: 0.8686392821226889\n",
      "---- n features = 15\n",
      "ACC: 0.8715092442175292\n",
      "---- n features = 16\n",
      "ACC: 0.877989221526005\n",
      "---- n features = 17\n",
      "ACC: 0.8772388540780278\n",
      "---- n features = 18\n",
      "ACC: 0.8772311183311416\n",
      "---- n features = 19\n",
      "ACC: 0.8829942497614811\n",
      "---- n features = 20\n",
      "ACC: 0.8772491684072097\n",
      "---- n features = 21\n",
      "ACC: 0.8901859157835021\n",
      "---- n features = 22\n",
      "ACC: 0.8916041360460019\n",
      "---- n features = 23\n",
      "ACC: 0.8908821330032748\n",
      "---- n features = 24\n",
      "ACC: 0.8901627085428432\n",
      "---- n features = 25\n",
      "ACC: 0.8865862148990485\n",
      "---- n features = 26\n",
      "ACC: 0.8880070137438437\n",
      "---- n features = 27\n",
      "ACC: 0.8908769758386839\n",
      "---- n features = 28\n",
      "ACC: 0.8865501147469121\n",
      "---- n features = 29\n",
      "ACC: 0.8923235605064337\n",
      "---- n features = 30\n",
      "ACC: 0.8966401072690235\n",
      "---- n features = 31\n",
      "ACC: 0.8937624094272969\n",
      "---- n features = 32\n",
      "ACC: 0.8980609061138187\n",
      "---- n features = 33\n",
      "ACC: 0.8973621103117505\n",
      "---- n features = 34\n",
      "ACC: 0.9038214589618627\n",
      "---- n features = 35\n",
      "ACC: 0.8980531703669323\n",
      "---- n features = 36\n",
      "ACC: 0.8959000541502282\n",
      "---- n features = 37\n",
      "ACC: 0.894456048064774\n",
      "---- n features = 38\n",
      "ACC: 0.9009179752971817\n",
      "---- n features = 39\n",
      "ACC: 0.9081405843067483\n",
      "---- n features = 40\n",
      "ACC: 0.9031020345014312\n",
      "---- n features = 41\n",
      "ACC: 0.90380598746809\n",
      "---- n features = 42\n",
      "ACC: 0.9016657641628634\n",
      "---- n features = 43\n",
      "ACC: 0.9088032799566799\n",
      "---- n features = 44\n",
      "ACC: 0.9066707923983394\n",
      "---- n features = 45\n",
      "ACC: 0.9124029808411336\n",
      "---- n features = 46\n",
      "ACC: 0.9052216291482944\n",
      "---- n features = 47\n",
      "ACC: 0.9073773239472936\n",
      "---- n features = 48\n",
      "ACC: 0.9088290657796343\n",
      "---- n features = 49\n",
      "ACC: 0.908792965627498\n",
      "---- n features = 50\n",
      "ACC: 0.9073876382764755\n",
      "---- n features = 51\n",
      "ACC: 0.9074005311879528\n",
      "---- n features = 52\n",
      "ACC: 0.9059694180139759\n",
      "---- n features = 53\n",
      "ACC: 0.9088419586911115\n",
      "---- n features = 54\n",
      "ACC: 0.9124210309172017\n",
      "---- n features = 55\n",
      "ACC: 0.9174415306464507\n",
      "---- n features = 56\n",
      "ACC: 0.9145896186276786\n",
      "---- n features = 57\n",
      "ACC: 0.9109950749078155\n",
      "---- n features = 58\n",
      "ACC: 0.9044970475232716\n",
      "---- n features = 59\n",
      "ACC: 0.9131585054537016\n",
      "---- n features = 60\n",
      "ACC: 0.913859879838065\n",
      "---- n features = 61\n",
      "ACC: 0.9109873391609293\n",
      "---- n features = 62\n",
      "ACC: 0.9145896186276786\n",
      "---- n features = 63\n",
      "ACC: 0.9159975245609964\n",
      "---- n features = 64\n",
      "ACC: 0.9088368015265207\n",
      "---- n features = 65\n",
      "ACC: 0.914558675640133\n",
      "---- n features = 66\n",
      "ACC: 0.9109821819963384\n",
      "---- n features = 67\n",
      "ACC: 0.9110053892369976\n",
      "---- n features = 68\n",
      "ACC: 0.9109899177432247\n",
      "---- n features = 69\n",
      "ACC: 0.9153167788349965\n",
      "---- n features = 70\n",
      "ACC: 0.9181583765245869\n",
      "---- n features = 71\n",
      "ACC: 0.9081276913952708\n",
      "---- n features = 72\n",
      "ACC: 0.91170160645677\n",
      "---- n features = 73\n",
      "ACC: 0.915285835847451\n",
      "---- n features = 74\n",
      "ACC: 0.9116938707098837\n",
      "---- n features = 75\n",
      "ACC: 0.9196049611923364\n",
      "---- n features = 76\n",
      "ACC: 0.9167298419329054\n",
      "---- n features = 77\n",
      "ACC: 0.9131430339599287\n",
      "---- n features = 78\n",
      "ACC: 0.9131224053015652\n",
      "---- n features = 79\n",
      "ACC: 0.9124210309172017\n",
      "---- n features = 80\n",
      "ACC: 0.9181764266006549\n",
      "---- n features = 81\n",
      "ACC: 0.9181557979422912\n",
      "---- n features = 82\n",
      "ACC: 0.9131378767953379\n",
      "---- n features = 83\n",
      "ACC: 0.9167272633506098\n",
      "---- n features = 84\n",
      "ACC: 0.9224465588819267\n",
      "---- n features = 85\n",
      "ACC: 0.9131120909723833\n",
      "---- n features = 86\n",
      "ACC: 0.9224775018694722\n",
      "---- n features = 87\n",
      "ACC: 0.9181274335370413\n",
      "---- n features = 88\n",
      "ACC: 0.9188649080735413\n",
      "---- n features = 89\n",
      "ACC: 0.9167246847683144\n",
      "---- n features = 90\n",
      "ACC: 0.9138495655088832\n",
      "---- n features = 91\n",
      "ACC: 0.9145689899693149\n",
      "---- n features = 92\n",
      "ACC: 0.9167349990974962\n",
      "---- n features = 93\n",
      "ACC: 0.913155926871406\n",
      "---- n features = 94\n",
      "ACC: 0.9174337948995642\n",
      "---- n features = 95\n",
      "ACC: 0.9188881153142002\n",
      "---- n features = 96\n",
      "ACC: 0.9167092132745417\n",
      "---- n features = 97\n",
      "ACC: 0.9195817539516774\n",
      "---- n features = 98\n",
      "ACC: 0.9217477630798587\n",
      "---- n features = 99\n",
      "ACC: 0.9181712694360641\n",
      "---- n features = 100\n",
      "ACC: 0.9224800804517678\n",
      "---- n features = 101\n",
      "ACC: 0.9203063355767\n",
      "---- n features = 102\n",
      "ACC: 0.9210412315309042\n",
      "---- n features = 103\n",
      "ACC: 0.9195765967870864\n",
      "---- n features = 104\n",
      "ACC: 0.9145921972099741\n",
      "---- n features = 105\n",
      "ACC: 0.9152755215182694\n",
      "---- n features = 106\n",
      "ACC: 0.9181661122714733\n",
      "---- n features = 107\n",
      "ACC: 0.9203063355766998\n",
      "---- n features = 108\n",
      "ACC: 0.9152909930120419\n",
      "---- n features = 109\n",
      "ACC: 0.9160052603078828\n",
      "---- n features = 110\n",
      "ACC: 0.9202985998298135\n",
      "---- n features = 111\n",
      "ACC: 0.9239215079549263\n",
      "---- n features = 112\n",
      "ACC: 0.9110002320724068\n",
      "---- n features = 113\n",
      "ACC: 0.9231737190892447\n",
      "---- n features = 114\n",
      "ACC: 0.9188700652381321\n",
      "---- n features = 115\n",
      "ACC: 0.9203114927412909\n",
      "---- n features = 116\n",
      "ACC: 0.9167272633506098\n",
      "---- n features = 117\n",
      "ACC: 0.9188674866558367\n",
      "---- n features = 118\n",
      "ACC: 0.9217529202444495\n",
      "---- n features = 119\n",
      "ACC: 0.9195894896985637\n",
      "---- n features = 120\n",
      "ACC: 0.9138547226734742\n",
      "---- n features = 121\n",
      "ACC: 0.9181557979422914\n",
      "---- n features = 122\n",
      "ACC: 0.9181815837652458\n",
      "---- n features = 123\n",
      "ACC: 0.9203140713235862\n",
      "---- n features = 124\n",
      "ACC: 0.9160104174724737\n",
      "---- n features = 125\n",
      "ACC: 0.9217297130037906\n",
      "---- n features = 126\n",
      "ACC: 0.9159949459787008\n",
      "---- n features = 127\n",
      "ACC: 0.9181970552590185\n",
      "---- n features = 128\n",
      "ACC: 0.9203089141589954\n",
      "---- n features = 129\n",
      "ACC: 0.919579175369382\n",
      "---- n features = 130\n",
      "ACC: 0.921028338619427\n",
      "---- n features = 131\n",
      "ACC: 0.9232020834944947\n",
      "---- n features = 132\n",
      "ACC: 0.9195946468631547\n",
      "---- n features = 133\n",
      "ACC: 0.9203243856527681\n",
      "---- n features = 134\n",
      "ACC: 0.923214976405972\n",
      "---- n features = 135\n",
      "ACC: 0.9160129960547693\n",
      "---- n features = 136\n",
      "ACC: 0.9203321213996544\n",
      "---- n features = 137\n",
      "ACC: 0.917438952064155\n",
      "---- n features = 138\n",
      "ACC: 0.923181454836131\n",
      "---- n features = 139\n",
      "ACC: 0.921028338619427\n",
      "---- n features = 140\n",
      "ACC: 0.9195894896985637\n",
      "---- n features = 141\n",
      "ACC: 0.9231866120007218\n",
      "---- n features = 142\n",
      "ACC: 0.9246383538330626\n",
      "---- n features = 143\n",
      "ACC: 0.918875222402723\n",
      "---- n features = 144\n",
      "ACC: 0.91959722544545\n",
      "---- n features = 145\n",
      "ACC: 0.9253371496351306\n",
      "---- n features = 146\n",
      "ACC: 0.920301178412109\n",
      "---- n features = 147\n",
      "ACC: 0.9210257600371315\n",
      "---- n features = 148\n",
      "ACC: 0.9195817539516774\n",
      "---- n features = 149\n",
      "ACC: 0.9210206028725405\n",
      "---- n features = 150\n",
      "ACC: 0.9217503416621542\n",
      "---- n features = 151\n",
      "ACC: 0.9181506407777004\n",
      "---- n features = 152\n",
      "ACC: 0.9239086150434492\n",
      "---- n features = 153\n",
      "ACC: 0.926781155720585\n",
      "Optimal number of features:  153\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:  ['x0' 'x1' 'x2' 'x3' 'x4' 'x5' 'x6' 'x7' 'x8' 'x9' 'x10' 'x11' 'x12' 'x13'\n",
      " 'x14' 'x15' 'x16' 'x17' 'x18' 'x19' 'x20' 'x21' 'x22' 'x23' 'x24' 'x25'\n",
      " 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32' 'x33' 'x34' 'x35' 'x36' 'x37'\n",
      " 'x38' 'x39' 'x40' 'x41' 'x42' 'x43' 'x44' 'x45' 'x46' 'x47' 'x48' 'x49'\n",
      " 'x50' 'x51' 'x52' 'x53' 'x54' 'x55' 'x56' 'x57' 'x58' 'x59' 'x60' 'x61'\n",
      " 'x62' 'x63' 'x64' 'x65' 'x66' 'x67' 'x68' 'x69' 'x70' 'x71' 'x72' 'x73'\n",
      " 'x74' 'x75' 'x76' 'x77' 'x78' 'x79' 'x80' 'x81' 'x82' 'x83' 'x84' 'x85'\n",
      " 'x86' 'x87' 'x88' 'x89' 'x90' 'x91' 'x92' 'x93' 'x94' 'x95' 'x96' 'x97'\n",
      " 'x98' 'x99' 'x100' 'x101' 'x102' 'x103' 'x104' 'x105' 'x106' 'x107'\n",
      " 'x108' 'x109' 'x110' 'x111' 'x112' 'x113' 'x114' 'x115' 'x116' 'x117'\n",
      " 'x118' 'x119' 'x120' 'x121' 'x122' 'x123' 'x124' 'x125' 'x126' 'x127'\n",
      " 'x128' 'x129' 'x130' 'x131' 'x132' 'x133' 'x134' 'x135' 'x136' 'x137'\n",
      " 'x138' 'x139' 'x140' 'x141' 'x142' 'x143' 'x144' 'x145' 'x146' 'x147'\n",
      " 'x148' 'x149' 'x150' 'x151' 'x152']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find optimal number of features using cross-validation filter\n",
    "################################################################################\n",
    "print(\"----- Optimal selection of number of features -----\")\n",
    "n = x.shape[1]  \n",
    "n_feats = [i for i in range(1, n + 1)]\n",
    "\n",
    "print(my_array)\n",
    "\n",
    "acc_nfeat = []\n",
    "for n_feat in n_feats:\n",
    "    print('---- n features =', n_feat)\n",
    "    acc_cv = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        # Training phase\n",
    "        x_train = x[train_index, :]\n",
    "        y_train = y[train_index]\n",
    "        clf_cv = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "        fselection_cv = SelectKBest(f_classif, k = n_feat) #SVC(kernel = 'linear')\n",
    "        fselection_cv.fit(x_train, y_train)\n",
    "        x_train = fselection_cv.transform(x_train)\n",
    "        clf_cv.fit(x_train, y_train)\n",
    "        # Test phase\n",
    "        x_test = fselection_cv.transform(x[test_index, :])\n",
    "        y_test = y[test_index]\n",
    "        y_pred = clf_cv.predict(x_test)\n",
    "        acc_i = accuracy_score(y_test, y_pred)\n",
    "        acc_cv.append(acc_i)\n",
    "    acc = np.average(acc_cv)\n",
    "    acc_nfeat.append(acc)\n",
    "    print('ACC:', acc)\n",
    "    \n",
    "opt_index = np.argmax(acc_nfeat)\n",
    "opt_features = n_feats[opt_index]\n",
    "print(\"Optimal number of features: \", opt_features)\n",
    "plt.plot(n_feats, acc_nfeat)\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "# Fit model with optimal number of features\n",
    "clf = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "fselection = SelectKBest(f_classif, k = opt_features)\n",
    "fselection.fit(x, y)\n",
    "print(\"Selected features: \", fselection.get_feature_names_out())\n",
    "xBestFilter = fselection.get_feature_names_out()\n",
    "x_transformed = fselection.transform(x)\n",
    "clf.fit(x_transformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    5. Repite el paso anterior, pero para un método de selección de características de tipo Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- Optimal selection of number of features -----\")\n",
    "n = x.shape[1]  \n",
    "n_feats = [i for i in range(1, n + 1)]\n",
    "acc_nfeat = []\n",
    "\n",
    "for n_feat in n_feats:\n",
    "\n",
    "    print('---- n features =', n_feat)\n",
    "    acc_cv = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "\n",
    "        # Training phase\n",
    "        x_train = x[train_index, :]\n",
    "        y_train = y[train_index]\n",
    "        clf_cv = SVC(kernel = 'linear')\n",
    "        fselection_cv = SequentialFeatureSelector(clf_cv,\n",
    "        n_features_to_select=n_feat)\n",
    "        fselection_cv.fit(x_train, y_train)\n",
    "        x_train = fselection_cv.transform(x_train)\n",
    "        clf_cv.fit(x_train, y_train)\n",
    "        # Test phase\n",
    "        x_test = fselection_cv.transform(x[test_index, :])\n",
    "        y_test = y[test_index]\n",
    "        y_pred = clf_cv.predict(x_test)\n",
    "        acc_i = accuracy_score(y_test, y_pred)\n",
    "        acc_cv.append(acc_i)\n",
    "    acc = np.average(acc_cv)\n",
    "    acc_nfeat.append(acc)\n",
    "    print('ACC:', acc)\n",
    "\n",
    "opt_index = np.argmax(acc_nfeat)\n",
    "opt_features = n_feats[opt_index]\n",
    "print(\"Optimal number of features: \", opt_features)\n",
    "plt.plot(n_feats, acc_nfeat)\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Fit model with optimal number of features\n",
    "clf = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "fselection = SequentialFeatureSelector(clf, n_features_to_select = opt_features)\n",
    "fselection.fit(x, y)\n",
    "print(\"Selected features: \", fselection.get_feature_names_out())\n",
    "x_transformed = fselection.transform(x)\n",
    "clf.fit(x_transformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este metodo no funciono debido a que se tarda mucho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    6. Repite el paso 4, pero para un método de selección de características de tipo Filter-Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- Optimal selection of number of features -----\")\n",
    "n = x.shape[1]  \n",
    "n_feats = [i for i in range(1, n + 1)]\n",
    "acc_nfeat = []\n",
    "\n",
    "for n_feat in n_feats:\n",
    "    print('---- n features =', n_feat)\n",
    "    acc_cv = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "\n",
    "        # Training phase\n",
    "        x_train = x[train_index, :]\n",
    "        y_train = y[train_index]\n",
    "        clf_cv = SVC(kernel = 'linear')\n",
    "        fselection_cv = RFE(clf_cv, n_features_to_select=n_feat)\n",
    "        fselection_cv.fit(x_train, y_train)\n",
    "        x_train = fselection_cv.transform(x_train)\n",
    "        clf_cv.fit(x_train, y_train)\n",
    "        # Test phase\n",
    "        x_test = fselection_cv.transform(x[test_index, :])\n",
    "        y_test = y[test_index]\n",
    "        y_pred = clf_cv.predict(x_test)\n",
    "        acc_i = accuracy_score(y_test, y_pred)\n",
    "        acc_cv.append(acc_i)\n",
    "\n",
    "    acc = np.average(acc_cv)\n",
    "    acc_nfeat.append(acc)\n",
    "    print('ACC:', acc)\n",
    "\n",
    "opt_index = np.argmax(acc_nfeat)\n",
    "opt_features = n_feats[opt_index]\n",
    "print(\"Optimal number of features: \", opt_features)\n",
    "plt.plot(n_feats, acc_nfeat)\n",
    "plt.xlabel(\"features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Fit model with optimal number of features\n",
    "clf = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "fselection = RFE(clf, n_features_to_select = opt_features)\n",
    "fselection.fit(x, y)\n",
    "print(\"Selected features: \", fselection.get_feature_names_out())\n",
    "x_transformed = fselection.transform(x)\n",
    "clf.fit(x_transformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este metodo no funciono debido a que se tarda mucho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        7. Escoge alguna de las técnicas de selección de características que probaste con anteioridad, y con el número óptimo de características encontrado, prepara tu modelo para producción haciendo lo siguiente:\n",
    "\n",
    "        Aplica el método de selección de características con todos los datos.\n",
    "        Ajusta el modelo con las características encontradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La unica tecnica de seleccion de carateristicas que funciono con el modelo fue el de filter, con un numero de 153 variables, la cantidad total de variables regresoras, sin embargo para n features de 55 se obtuvo una accurracy de 0.9174415306464507, por lo que se decidio hacer el ejercicio con este numero de variables al ser casi 1/3 de la cantidad mas optima y solo tener una diferencia de menos del 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:  ['x11' 'x12' 'x13' 'x14' 'x16' 'x17' 'x18' 'x19' 'x20' 'x21' 'x22' 'x23'\n",
      " 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32' 'x38' 'x39' 'x55' 'x56' 'x60'\n",
      " 'x61' 'x62' 'x64' 'x65' 'x66' 'x67' 'x76' 'x77' 'x78' 'x79' 'x80' 'x81'\n",
      " 'x82' 'x83' 'x87' 'x88' 'x89' 'x90' 'x91' 'x92' 'x112' 'x113' 'x116'\n",
      " 'x117' 'x123' 'x124' 'x125' 'x126' 'x127' 'x140' 'x141']\n",
      "----- RBF-SVM -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.83      0.79       278\n",
      "           2       0.96      0.93      0.94      1115\n",
      "\n",
      "    accuracy                           0.91      1393\n",
      "   macro avg       0.86      0.88      0.87      1393\n",
      "weighted avg       0.92      0.91      0.91      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit model with optimal number of features\n",
    "clf = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "fselection = SelectKBest(f_classif, k = 55)\n",
    "fselection.fit(x, y)\n",
    "print(\"Selected features: \", fselection.get_feature_names_out())\n",
    "xBestFilter = fselection.get_feature_names_out()\n",
    "x_transformed = fselection.transform(x)\n",
    "#clf.fit(x_transformed, y)\n",
    "print('----- RBF-SVM -----')\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "clf = SVC(kernel = 'rbf', class_weight='balanced')\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(x_transformed, y):\n",
    "    # Training phase\n",
    "    x_train = x_transformed[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Test phase\n",
    "    x_test = x_transformed[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Contesta las siguientes preguntas:\n",
    "\n",
    "    ¿Qué pasa si no se considera el problema de tener datos desbalanceados para este caso? ¿Por qué?\n",
    "\n",
    "Para este conjunto de datos, los datos están desbalanceados. Si no se aplica una técnica de balanceo, los resultados podrían estar sesgados. En este caso, tenemos casi 5 veces más una clase, por lo tanto, esta tendría una gran precisión, mientras que la otra clase los resultados serían incorrectos. Esto se puede observar en el recall del modelo. Un recall balanceado y alto es lo esperado, aunque la precisión baje. \n",
    "\n",
    "    De todos los clasificadores, ¿cuál o cuales consideras que son adecuados para los datos? ¿Qué propiedades tienen dichos modelos que los hacen apropiados para los datos? Argumenta tu respuesta.\n",
    "\n",
    "El clasificador que proporcionó los mejores resultados fue el de RBF, aunque los modelos de regresión logística y lineal también arrojaron resultados considerables. Sin embargo, la regresión logística y lineal tuvieron una baja precisión para la categoría '1'. Esto puede deberse al método que apliqué para el balanceo, ya que solo dio un valor bajo en esa métrica. La efectividad del clasificador RBF podría deberse a que es un modelo no lineal, lo que lo hace más adecuado para el conjunto de datos, especialmente cuando se comparan los resultados con los modelos lineales.\n",
    "\n",
    "    ¿Es posible reducir la dimensionalidad del problema sin perder rendimiento en el modelo? ¿Por qué?\n",
    "\n",
    "Sí, es posible reducir la dimensionalidad del problema sin perder rendimiento en el modelo, aunque en este conjunto de datos, la mejor dimensionalidad se obtuvo utilizando todas las variables independientes. Sin embargo, al observar la gráfica del método Filter, se puede notar que hay un punto en el cual el número de características deja de aumentar significativamente. En otros conjuntos de datos, reducir la dimensionalidad puede ser beneficioso, ya que disminuye la complejidad del modelo y puede ayudar a evitar el sobreajuste.\n",
    "\n",
    "    ¿Qué método de selección de características consideras el más adecuado para este caso? ¿Por qué?\n",
    "\n",
    "Para este caso, el mejor y único método que pude aplicar fue el método Filter, debido a que no depende del modelo en sí, sino de las características de forma independiente. Esto reduce significativamente el costo computacional. Los otros dos métodos, Wrapper y Filter-Wrapper, podrían haberse calculado, pero dado que dependen del modelo, el costo computacional es mucho más alto y podría llevar horas obtener resultados.\n",
    "\n",
    "    Si quisieras mejorar el rendimiento de tus modelos, ¿qué más se podría hacer?\n",
    "\n",
    "Hay varias opciones para mejorar el modelo, como esperar el tiempo suficiente para que los métodos Wrapper y Filter-Wrapper proporcionen resultados, además de probar diferentes métodos de balanceo. También se podría evaluar diferentes hiperparámetros, como el número de k vecinos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "En este ejercicio trabajarás con datos que vienen de un experimento en el que se midió actividad muscular con la técnica de la Electromiografía en el brazo derecho de varios participantes cuando éstos realizaban un movimiento con la mano entre siete posible (Flexionar hacia arriba, Flexionar hacia abajo, Cerrar la mano, Estirar la mano, Abrir la mano, Coger un objeto, No moverse). Al igual que en el ejercicio anterior, los datos se cargan con la función loadtxt de numpy Links to an external site.. A su vez, la primera columna corresponde a la clase (1, 2, 3, 4, 5, 6, y 7), la segunda columna se ignora, y el resto de las columnas indican las variables que se calcularon de la respuesta muscular. El archivo de datos con el que trabajarás depende de tu matrícula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>V623</th>\n",
       "      <th>V624</th>\n",
       "      <th>V625</th>\n",
       "      <th>V626</th>\n",
       "      <th>V627</th>\n",
       "      <th>V628</th>\n",
       "      <th>V629</th>\n",
       "      <th>V630</th>\n",
       "      <th>V631</th>\n",
       "      <th>V632</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.379487</td>\n",
       "      <td>0.894114</td>\n",
       "      <td>0.597094</td>\n",
       "      <td>-1.114525</td>\n",
       "      <td>0.176757</td>\n",
       "      <td>0.217511</td>\n",
       "      <td>0.957966</td>\n",
       "      <td>0.653276</td>\n",
       "      <td>0.467849</td>\n",
       "      <td>...</td>\n",
       "      <td>1.756593</td>\n",
       "      <td>-0.451745</td>\n",
       "      <td>1.794924</td>\n",
       "      <td>1.176389</td>\n",
       "      <td>-0.581349</td>\n",
       "      <td>1.466926</td>\n",
       "      <td>0.219139</td>\n",
       "      <td>-0.427162</td>\n",
       "      <td>1.545800</td>\n",
       "      <td>1.947798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.086426</td>\n",
       "      <td>0.058724</td>\n",
       "      <td>-0.361846</td>\n",
       "      <td>-1.202523</td>\n",
       "      <td>0.153521</td>\n",
       "      <td>-0.108678</td>\n",
       "      <td>-0.328419</td>\n",
       "      <td>-0.188628</td>\n",
       "      <td>-0.114548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091445</td>\n",
       "      <td>-0.673981</td>\n",
       "      <td>0.194601</td>\n",
       "      <td>0.247911</td>\n",
       "      <td>-1.135336</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>-0.134775</td>\n",
       "      <td>-0.451924</td>\n",
       "      <td>0.076792</td>\n",
       "      <td>-0.187741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.120878</td>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.076013</td>\n",
       "      <td>-0.602122</td>\n",
       "      <td>-0.666107</td>\n",
       "      <td>-0.630813</td>\n",
       "      <td>-0.271406</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>-0.975923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.680615</td>\n",
       "      <td>-0.801010</td>\n",
       "      <td>-0.350549</td>\n",
       "      <td>0.302109</td>\n",
       "      <td>-1.757509</td>\n",
       "      <td>0.204798</td>\n",
       "      <td>-1.041059</td>\n",
       "      <td>-0.615934</td>\n",
       "      <td>0.351723</td>\n",
       "      <td>-0.737440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.351626</td>\n",
       "      <td>0.895506</td>\n",
       "      <td>0.354001</td>\n",
       "      <td>-0.984980</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>0.888001</td>\n",
       "      <td>-0.337790</td>\n",
       "      <td>0.090245</td>\n",
       "      <td>0.560630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047266</td>\n",
       "      <td>-0.775160</td>\n",
       "      <td>1.170296</td>\n",
       "      <td>0.380971</td>\n",
       "      <td>-1.274298</td>\n",
       "      <td>1.070921</td>\n",
       "      <td>-0.134358</td>\n",
       "      <td>-0.370253</td>\n",
       "      <td>1.578293</td>\n",
       "      <td>0.371553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.215150</td>\n",
       "      <td>-0.019875</td>\n",
       "      <td>-0.589697</td>\n",
       "      <td>-2.876677</td>\n",
       "      <td>-0.932048</td>\n",
       "      <td>-0.802137</td>\n",
       "      <td>-0.033036</td>\n",
       "      <td>-1.120116</td>\n",
       "      <td>-1.523562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344517</td>\n",
       "      <td>-1.042312</td>\n",
       "      <td>-0.053075</td>\n",
       "      <td>0.072212</td>\n",
       "      <td>-1.613424</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>-1.102570</td>\n",
       "      <td>-0.834125</td>\n",
       "      <td>1.310845</td>\n",
       "      <td>-0.121053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>7</td>\n",
       "      <td>-4.764398</td>\n",
       "      <td>-5.173067</td>\n",
       "      <td>-3.162772</td>\n",
       "      <td>-4.354816</td>\n",
       "      <td>-3.841628</td>\n",
       "      <td>-2.819353</td>\n",
       "      <td>-4.607093</td>\n",
       "      <td>-3.640769</td>\n",
       "      <td>-4.109369</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.081282</td>\n",
       "      <td>-2.195790</td>\n",
       "      <td>-3.918950</td>\n",
       "      <td>-5.518796</td>\n",
       "      <td>-6.302458</td>\n",
       "      <td>-4.503126</td>\n",
       "      <td>-6.134198</td>\n",
       "      <td>-1.961771</td>\n",
       "      <td>-3.668271</td>\n",
       "      <td>-3.400192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>7</td>\n",
       "      <td>-5.315286</td>\n",
       "      <td>-5.195915</td>\n",
       "      <td>-3.281644</td>\n",
       "      <td>-4.474814</td>\n",
       "      <td>-4.960719</td>\n",
       "      <td>-3.396900</td>\n",
       "      <td>-5.018654</td>\n",
       "      <td>-6.814412</td>\n",
       "      <td>-4.903666</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.953465</td>\n",
       "      <td>-1.723648</td>\n",
       "      <td>-4.715917</td>\n",
       "      <td>-5.151268</td>\n",
       "      <td>-7.740101</td>\n",
       "      <td>-6.387718</td>\n",
       "      <td>-7.075405</td>\n",
       "      <td>-1.502264</td>\n",
       "      <td>-3.691837</td>\n",
       "      <td>-6.123619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>7</td>\n",
       "      <td>-6.609802</td>\n",
       "      <td>-5.108913</td>\n",
       "      <td>-2.907823</td>\n",
       "      <td>-4.461101</td>\n",
       "      <td>-4.676772</td>\n",
       "      <td>-2.563865</td>\n",
       "      <td>-5.459646</td>\n",
       "      <td>-6.554167</td>\n",
       "      <td>-4.501447</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.644879</td>\n",
       "      <td>-1.577503</td>\n",
       "      <td>-5.206570</td>\n",
       "      <td>-5.673828</td>\n",
       "      <td>-7.900117</td>\n",
       "      <td>-8.063860</td>\n",
       "      <td>-6.160683</td>\n",
       "      <td>-1.355751</td>\n",
       "      <td>-3.993137</td>\n",
       "      <td>-5.957840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>7</td>\n",
       "      <td>-6.376412</td>\n",
       "      <td>-5.977268</td>\n",
       "      <td>-3.832173</td>\n",
       "      <td>-4.736427</td>\n",
       "      <td>-5.609423</td>\n",
       "      <td>-3.827419</td>\n",
       "      <td>-5.697619</td>\n",
       "      <td>-6.528336</td>\n",
       "      <td>-5.751097</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.152191</td>\n",
       "      <td>-1.981974</td>\n",
       "      <td>-6.039118</td>\n",
       "      <td>-5.610517</td>\n",
       "      <td>-7.821325</td>\n",
       "      <td>-7.349157</td>\n",
       "      <td>-7.417253</td>\n",
       "      <td>-1.772635</td>\n",
       "      <td>-4.038534</td>\n",
       "      <td>-5.921882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>7</td>\n",
       "      <td>-5.587141</td>\n",
       "      <td>-5.655471</td>\n",
       "      <td>-2.825541</td>\n",
       "      <td>-4.253410</td>\n",
       "      <td>-4.736551</td>\n",
       "      <td>-2.840443</td>\n",
       "      <td>-5.271600</td>\n",
       "      <td>-6.965017</td>\n",
       "      <td>-4.730170</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.966530</td>\n",
       "      <td>-1.220673</td>\n",
       "      <td>-5.992121</td>\n",
       "      <td>-5.779087</td>\n",
       "      <td>-8.433890</td>\n",
       "      <td>-7.692398</td>\n",
       "      <td>-7.485458</td>\n",
       "      <td>-1.006292</td>\n",
       "      <td>-4.407043</td>\n",
       "      <td>-6.037502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 631 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     V1        V3        V4        V5        V6        V7        V8        V9  \\\n",
       "0     1  1.379487  0.894114  0.597094 -1.114525  0.176757  0.217511  0.957966   \n",
       "1     1 -0.086426  0.058724 -0.361846 -1.202523  0.153521 -0.108678 -0.328419   \n",
       "2     1  0.120878  0.279174 -0.076013 -0.602122 -0.666107 -0.630813 -0.271406   \n",
       "3     1 -0.351626  0.895506  0.354001 -0.984980  0.040372  0.888001 -0.337790   \n",
       "4     1  0.215150 -0.019875 -0.589697 -2.876677 -0.932048 -0.802137 -0.033036   \n",
       "..   ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "625   7 -4.764398 -5.173067 -3.162772 -4.354816 -3.841628 -2.819353 -4.607093   \n",
       "626   7 -5.315286 -5.195915 -3.281644 -4.474814 -4.960719 -3.396900 -5.018654   \n",
       "627   7 -6.609802 -5.108913 -2.907823 -4.461101 -4.676772 -2.563865 -5.459646   \n",
       "628   7 -6.376412 -5.977268 -3.832173 -4.736427 -5.609423 -3.827419 -5.697619   \n",
       "629   7 -5.587141 -5.655471 -2.825541 -4.253410 -4.736551 -2.840443 -5.271600   \n",
       "\n",
       "          V10       V11  ...      V623      V624      V625      V626  \\\n",
       "0    0.653276  0.467849  ...  1.756593 -0.451745  1.794924  1.176389   \n",
       "1   -0.188628 -0.114548  ...  0.091445 -0.673981  0.194601  0.247911   \n",
       "2    0.026564 -0.975923  ... -0.680615 -0.801010 -0.350549  0.302109   \n",
       "3    0.090245  0.560630  ...  1.047266 -0.775160  1.170296  0.380971   \n",
       "4   -1.120116 -1.523562  ... -0.344517 -1.042312 -0.053075  0.072212   \n",
       "..        ...       ...  ...       ...       ...       ...       ...   \n",
       "625 -3.640769 -4.109369  ... -5.081282 -2.195790 -3.918950 -5.518796   \n",
       "626 -6.814412 -4.903666  ... -5.953465 -1.723648 -4.715917 -5.151268   \n",
       "627 -6.554167 -4.501447  ... -5.644879 -1.577503 -5.206570 -5.673828   \n",
       "628 -6.528336 -5.751097  ... -6.152191 -1.981974 -6.039118 -5.610517   \n",
       "629 -6.965017 -4.730170  ... -6.966530 -1.220673 -5.992121 -5.779087   \n",
       "\n",
       "         V627      V628      V629      V630      V631      V632  \n",
       "0   -0.581349  1.466926  0.219139 -0.427162  1.545800  1.947798  \n",
       "1   -1.135336  0.329500 -0.134775 -0.451924  0.076792 -0.187741  \n",
       "2   -1.757509  0.204798 -1.041059 -0.615934  0.351723 -0.737440  \n",
       "3   -1.274298  1.070921 -0.134358 -0.370253  1.578293  0.371553  \n",
       "4   -1.613424  0.013676 -1.102570 -0.834125  1.310845 -0.121053  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "625 -6.302458 -4.503126 -6.134198 -1.961771 -3.668271 -3.400192  \n",
       "626 -7.740101 -6.387718 -7.075405 -1.502264 -3.691837 -6.123619  \n",
       "627 -7.900117 -8.063860 -6.160683 -1.355751 -3.993137 -5.957840  \n",
       "628 -7.821325 -7.349157 -7.417253 -1.772635 -4.038534 -5.921882  \n",
       "629 -8.433890 -7.692398 -7.485458 -1.006292 -4.407043 -6.037502  \n",
       "\n",
       "[630 rows x 631 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load database\n",
    "df = pd.read_csv('/home/alanv/Documents/7/omar/M_4.txt', delimiter='\\t', header=None)\n",
    "#dfA = np.loadtxt('/home/alanv/Documents/7/omar/P1_1.txt')\n",
    "#dfA.shape (1393, 155)\n",
    "column_names = [f\"V{i}\" for i in range(1, df.shape[1] + 1)]\n",
    "df.columns = column_names\n",
    "df.drop(['V2','V633'],inplace=True,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Determina si es necesario balancear los datos. En caso de que sea afirmativo, en todo este ejercicio tendrás que utilizar alguna estrategia para mitigar el problema de tener una muestra desbalanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_counts = df['V1'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "value_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Value Counts in dependient Variable')\n",
    "plt.xlabel('Unique Values')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo estos resultados, los datos no se necesitan balancear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Evalúa al menos 5 modelos de clasificación distintos utilizando validación cruzada, y determina cuál de ellos es el más efectivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializate x and y\n",
    "x = df.iloc[:, 1:].values\n",
    "y = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Linear SVM -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.97      0.97        90\n",
      "           2       0.81      0.87      0.84        90\n",
      "           3       0.93      0.87      0.90        90\n",
      "           4       0.91      0.91      0.91        90\n",
      "           5       0.92      0.93      0.93        90\n",
      "           6       0.98      0.90      0.94        90\n",
      "           7       0.89      0.96      0.92        90\n",
      "\n",
      "    accuracy                           0.91       630\n",
      "   macro avg       0.92      0.91      0.91       630\n",
      "weighted avg       0.92      0.91      0.91       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-----  Linear SVM -----\")\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "clf = SVC(kernel = 'linear')\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    # Training phase\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Test phase\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RBF-SVM -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.96      0.96        90\n",
      "           2       0.83      0.86      0.84        90\n",
      "           3       0.97      0.87      0.92        90\n",
      "           4       0.89      0.93      0.91        90\n",
      "           5       0.93      0.92      0.93        90\n",
      "           6       0.99      0.91      0.95        90\n",
      "           7       0.88      1.00      0.94        90\n",
      "\n",
      "    accuracy                           0.92       630\n",
      "   macro avg       0.92      0.92      0.92       630\n",
      "weighted avg       0.92      0.92      0.92       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----- RBF-SVM -----')\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "clf = SVC(kernel = 'rbf')\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    # Training phase\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Test phase\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "    \n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Decision tree -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.74      0.74        90\n",
      "           2       0.46      0.43      0.45        90\n",
      "           3       0.66      0.69      0.67        90\n",
      "           4       0.54      0.58      0.56        90\n",
      "           5       0.61      0.57      0.59        90\n",
      "           6       0.53      0.56      0.54        90\n",
      "           7       0.89      0.87      0.88        90\n",
      "\n",
      "    accuracy                           0.63       630\n",
      "   macro avg       0.63      0.63      0.63       630\n",
      "weighted avg       0.63      0.63      0.63       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "print('----- Decision tree -----')\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Random Forest -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.89      0.90        90\n",
      "           2       0.78      0.76      0.77        90\n",
      "           3       0.91      0.83      0.87        90\n",
      "           4       0.80      0.86      0.83        90\n",
      "           5       0.85      0.91      0.88        90\n",
      "           6       0.93      0.84      0.88        90\n",
      "           7       0.89      0.99      0.94        90\n",
      "\n",
      "    accuracy                           0.87       630\n",
      "   macro avg       0.87      0.87      0.87       630\n",
      "weighted avg       0.87      0.87      0.87       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print('----- Random Forest -----')\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.77337996,  1.23806631,  1.39077237, ...,  0.24421788,\n",
       "         1.03584738,  1.36823187],\n",
       "       [ 1.04505347,  0.77973814,  0.58552029, ...,  0.19442159,\n",
       "         0.22911131,  0.36371337],\n",
       "       [ 1.14805112,  0.90068564,  0.82554263, ..., -0.13540931,\n",
       "         0.38009527,  0.10514503],\n",
       "       ...,\n",
       "       [-2.19602907, -2.05543574, -1.55241885, ..., -1.62321124,\n",
       "        -2.00597423, -2.35043577],\n",
       "       [-2.0800714 , -2.53185053, -2.32862493, ..., -2.46158228,\n",
       "        -2.03090459, -2.33352179],\n",
       "       [-1.68792863, -2.35529938, -1.48332403, ..., -0.92043493,\n",
       "        -2.23327903, -2.38790727]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standarizise since metod is requesting it\n",
    "scaler= StandardScaler()\n",
    "df_estandard=scaler.fit_transform(df.iloc[:, 1:])\n",
    "df_estandard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Logistic Regression -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alanv/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/alanv/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/alanv/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/alanv/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.93      0.93        90\n",
      "           2       0.79      0.81      0.80        90\n",
      "           3       0.93      0.86      0.89        90\n",
      "           4       0.90      0.93      0.92        90\n",
      "           5       0.92      0.92      0.92        90\n",
      "           6       0.96      0.91      0.94        90\n",
      "           7       0.87      0.93      0.90        90\n",
      "\n",
      "    accuracy                           0.90       630\n",
      "   macro avg       0.90      0.90      0.90       630\n",
      "weighted avg       0.90      0.90      0.90       630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alanv/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "print('----- Logistic Regression -----')\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "for train_index, test_index in kf.split(df_estandard, y):\n",
    "    x_train = df_estandard[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = df_estandard[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Escoge al menos dos clasificadores que hayas evaluado en el paso anterior e identifica sus hiperparámetros. Lleva a cabo el proceso de validación cruzada anidada para evaluar los dos modelos con la selección óptima de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.01, 'class_weight': None, 'kernel': 'linear'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        90\n",
      "           2       1.00      1.00      1.00        90\n",
      "           3       1.00      0.97      0.98        90\n",
      "           4       1.00      1.00      1.00        90\n",
      "           5       1.00      1.00      1.00        90\n",
      "           6       1.00      0.98      0.99        90\n",
      "           7       0.95      1.00      0.97        90\n",
      "\n",
      "    accuracy                           0.99       630\n",
      "   macro avg       0.99      0.99      0.99       630\n",
      "weighted avg       0.99      0.99      0.99       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear'],  # Use only the linear kernel\n",
    "    'class_weight': [None, 'balanced'],  # Class weight options\n",
    "}\n",
    "\n",
    "# Create the SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Create the GridSearchCV object, cv = cross validation\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='f1_macro') \n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Print the classification report for the best model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(x)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_neighbors': 13}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.96      0.94        90\n",
      "           2       0.81      0.87      0.84        90\n",
      "           3       0.96      0.89      0.92        90\n",
      "           4       0.90      0.84      0.87        90\n",
      "           5       0.93      0.92      0.93        90\n",
      "           6       0.96      0.89      0.92        90\n",
      "           7       0.88      1.00      0.94        90\n",
      "\n",
      "    accuracy                           0.91       630\n",
      "   macro avg       0.91      0.91      0.91       630\n",
      "weighted avg       0.91      0.91      0.91       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['rbf'],  # Use only the rbf kernel\n",
    "    'class_weight': [None, 'balanced'],  # Class weight options\n",
    "}\n",
    "\n",
    "# Create the SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Create the GridSearchCV object, cv = cross validation\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': np.arange(1, 100)}, cv = 5)\n",
    "# Perform the grid search\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Print the classification report for the best model\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(x)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Prepara tus modelos para producción haciendo lo siguiente:\n",
    "\n",
    "    Opten los hiperparámetros óptimos utilizando todo el conjunto de datos con validación cruzada.\n",
    "    Con los hiperparámetros óptimos, ajusta el modelo con todos los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Model evaluation with cross_val_predict -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.91      0.92        90\n",
      "           2       0.71      0.81      0.76        90\n",
      "           3       0.90      0.84      0.87        90\n",
      "           4       0.84      0.80      0.82        90\n",
      "           5       0.92      0.86      0.89        90\n",
      "           6       0.90      0.83      0.87        90\n",
      "           7       0.88      1.00      0.94        90\n",
      "\n",
      "    accuracy                           0.87       630\n",
      "   macro avg       0.87      0.87      0.87       630\n",
      "weighted avg       0.87      0.87      0.87       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with cross_val_predict\n",
    "################################################################################\n",
    "print(\"----- Model evaluation with cross_val_predict -----\")\n",
    "clf = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': np.arange(1, 100)}, cv = 5)\n",
    "y_pred = cross_val_predict(clf, x, y, cv = 5)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Production model -----\n",
      "KNeighborsClassifier(n_neighbors=13)\n"
     ]
    }
   ],
   "source": [
    "# Production model\n",
    "################################################################################\n",
    "print(\"----- Production model -----\")\n",
    "clf = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': np.arange(1, 100)}, cv =\n",
    "5)\n",
    "clf.fit(x, y)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Contesta lo siguientes:\n",
    "\n",
    "    ¿Observas un problema en cuanto al balanceo de las clases? ¿Por qué?\n",
    "No se observa un problema de balanceo. Al graficar la cantidad de variables regresoras, todas tienen un valor similar. Además, cuando se prueban los modelos sin un método para corregir el desbalanceo, el valor del recall es alto y parecido entre todas las categorías.\n",
    "\n",
    "    ¿Qué modelo o modelos fueron efectivos para clasificar tus datos? ¿Observas algo especial sobre los modelos? Argumenta tu respuesta.\n",
    "\n",
    "Los mejores modelos fueron el RBF y el de regresión logística. Sin embargo, el modelo de regresión logística dio algunos warnings y se tuvieron que estandarizar los datos, como lo pedía la librería. Estos problemas persistieron debido a que este método se utiliza regularmente para clasificar entre dos categorías, pero al tener 7, podría causar conflictos al tratar de implementar otro método que lo permite, como Softmax.\n",
    "\n",
    "    ¿Observas alguna mejora importante al optimizar hiperparámetros? ¿Es el resultado que esperabas? Argumenta tu respuesta.\n",
    "\n",
    "Sí, hay una mejora importante al modificar el hiperparámetro de regularización (C). No es el resultado que esperaba, ya que da resultados casi perfectos, entre 0.99 y 1. Esto puede deberse a un sobreajuste, por lo que decidí utilizar el hiperparámetro de k vecinos, donde se obtiene un resultado ligeramente mejor que los anteriores modelos y un número de vecinos que se encuentra en un rango aceptable, considerando que el modelo tiene una variabilidad no muy grande.\n",
    "\n",
    "    ¿Qué inconvenientes hay al encontrar hiperparámetros? ¿Por qué?\n",
    "\n",
    "Uno de los mayores inconvenientes es el sobreajuste, ya que se entrena muy bien el modelo, pero solo para los datos del dataset. Cuando el modelo está en producción y se introducen nuevos datos, el modelo podría dar errores debido a que solo es bueno con los datos con los que fue entrenado.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
